# AIND Ephys Hybrid Benhmark
## aind-ephys-hybrid-benchmark

Electrophysiology analysis pipeline with [SpikeInterface](https://github.com/SpikeInterface/spikeinterface).

The pipeline is based on [Nextflow](https://www.nextflow.io/) and it includes the following steps:

- [job-dispatch](https://github.com/AllenNeuralDynamics/aind-ephys-job-dispatch/): generates a list of JSON files to be processed in parallel. Parallelization is performed over multiple probes and multiple shanks (e.g., for NP2-4shank probes). The steps from `hybrid-generation` to `spike-sorting` are run in parallel.
- [hybrid-generation](https://github.com/AllenNeuralDynamics/aind-ephys-hybrid-generationh/): generate hybrid recordings for each input JSON file
- [preprocessing](https://github.com/AllenNeuralDynamics/aind-ephys-preprocessing/): phase_shift, highpass filter, denoising (bad channel removal + common median reference ("cmr") or highpass spatial filter - "destripe"), and motion estimation (optionally correction)
- spike sorting: several spike sorters are tested:
  - [kilosort2.5](https://github.com/AllenNeuralDynamics/aind-ephys-spikesort-kilosort25/)
  - [kilosort4](https://github.com/AllenNeuralDynamics/aind-ephys-spikesort-kilosort4/)
  - [spykingcircus2](https://github.com/AllenNeuralDynamics/aind-ephys-spikesort-spykingcircus2/)
- [hybrid-evaluation](https://github.com/AllenNeuralDynamics/aind-ephys-hybrid-evaluation/): collect and evaluate results

Each step is run in a container and can be deployed on several platforms. See [Deployments](#deployments) for more details.

# Input


# Output


**`nextflow`**

All files generated by Nextflow are saved here


# Parameters

## Global parameters

In Nextflow, the The `-resume` argument enables the caching mechanism.

The following global parameters can be passed to the pipeline:

```bash
--n_jobs N_JOBS    For local deployment, how many jobs to run in parallel
```

## Process-specific parameters

Some steps of the pipeline accept additional parameters, that can be passed as follows:

```bash
--{step_name}_args "{args}"
```

The steps that accept additional arguments are:

### `job_dispatch_args`:

```bash
  ---concatenate        Whether to concatenate recordings (segments) or not. Default: False
  --split-groups        Whether to process different groups separately
  --debug               Whether to run in DEBUG mode
  --debug-duration DEBUG_DURATION
                        Duration of clipped recording in debug mode. Default is 30 seconds. Only used if debug is enabled
  --input {aind,spikeglx,nwb}
                        Which 'loader' to use (aind | spikeglx | nwb)
```

- `spikeglx`: the `DATA_PATH` should contain a SpikeGLX saved folder. It is recommended to add a `subject.json` and a `data_description.json` following the [aind-data-schema](https://aind-data-schema.readthedocs.io/en/latest/) specification, since these metadata are propagated to the NWB files.
- `openephys`: the `DATA_PATH` should contain an Open Ephys folder. It is recommended to add a `subject.json` and a `data_description.json` following the [aind-data-schema](https://aind-data-schema.readthedocs.io/en/latest/) specification, since these metadata are propagated to the NWB files.
- `nwb`: the `DATA_PATH` should contain an NWB file (both HDF5 and Zarr backend are supported).
- `aind`: data ingestion used at AIND. The `DATA_PATH` must contain an `ecephys` subfolder which in turn includes an `ecephys_clipped` (clipped Open Ephys folder) and an `ecephys_compressed` (compressed traces with Zarr). In addition, JSON file following the [aind-data-schema](https://aind-data-schema.readthedocs.io/en/latest/) are parsed to create processing and NWB metadata.

### `hybrid_generation_args`

```bash
  --min-amp MIN_AMP     Minimum amplitude to scale injected templates
  --max-amp MAX_AMP     Maximum amplitude to scale injected templates
  --min-depth-percentile MIN_DEPTH_PERCENTILE
                        Percentile of depths used as minimum depth
  --max-depth-percentile MAX_DEPTH_PERCENTILE
                        Percentile of depths used as maximum depth
  --num-units NUM_UNITS
                        Number of hybrid units for each case
  --num-cases NUM_CASES
                        Number of cases for each recording
  --skip-correct-motion
                        Whether to skip motion correction.

```

### `preprocessing_args`:

```bash
  --denoising {cmr,destripe}
                        Which denoising strategy to use. Can be 'cmr' or 'destripe'
  --filter-type {highpass,bandpass}
                        Which filter to use. Can be 'highpass' or 'bandpass'
  --no-remove-out-channels
                        Whether to remove out channels
  --no-remove-bad-channels
                        Whether to remove bad channels
  --max-bad-channel-fraction MAX_BAD_CHANNEL_FRACTION
                        Maximum fraction of bad channels to remove. If more than this fraction, processing is skipped
  --motion {skip,compute,apply}
                        How to deal with motion correction. Can be 'skip', 'compute', or 'apply'
  --motion-preset {dredge,dredge_fast,nonrigid_accurate,nonrigid_fast_and_accurate,rigid_fast,kilosort_like}
                        What motion preset to use. Supported presets are:
                        dredge, dredge_fast, nonrigid_accurate, nonrigid_fast_and_accurate, rigid_fast, kilosort_like.
  --t-start T_START     Start time of the recording in seconds (assumes recording starts at 0). 
                        This parameter is ignored in case of multi-segment or multi-block recordings.
                        Default is None (start of recording)
  --t-stop T_STOP       Stop time of the recording in seconds (assumes recording starts at 0). 
                        This parameter is ignored in case of multi-segment or multi-block recordings.
                        Default is None (end of recording)

```


# Deployments

## Local

> [!WARNING]
> While the pipeline can be deployed locally on a workstation or a server, it is recommended to 
> deploy it on a SLURM cluster or on a batch processing system (e.g., AWS batch).
> When deploying locally, the most recource-intensive processes (preprocessing, spike sorting, postprocessing) 
> are not parallelized to avoid overloading the system.
> This is achieved by setting the `maxForks 1` directive in such processes.

### Requirements

To deploy locally, you need to install:

- `nextflow`
- `docker`

Please checkout the [Nextflow](https://www.nextflow.io/docs/latest/install.html) and [Docker](https://docs.docker.com/engine/install/) installation instructions.


### Run

Clone this repo (`git clone https://github.com/AllenNeuralDynamics/aind-ephys-hybrid-benchmark.git`) and go to the 
`pipeline` folder. You will find a `main_local.nf`. This nextflow script is accompanied by the 
`nextflow_local.config` and can run on local workstations/machines.

To invoke the pipeline you can run the following command:

```bash
NXF_VER=22.10.8 DATA_PATH=$PWD/../data RESULTS_PATH=$PWD/../results \
    nextflow -C nextflow_local.config -log $RESULTS_PATH/nextflow/nextflow.log \
    run main_local.nf \
    --n_jobs 8 -resume
```

The `DATA_PATH` specifies the folder where the input files are located. 
The `RESULT_PATH` points to the output folder, where the data will be saved.
The `--n_jobs` argument specifies the number of parallel jobs to run.

Additional parameters can be passed as described in the [Parameters](#parameters) section.


### Example run command

As an example, here is how to run the pipeline on a SpikeGLX dataset in debug mode 
on a 120-second snippet of the recording with 16 jobs:

```bash
NXF_VER=22.10.8 DATA_PATH=path/to/data_spikeglx RESULTS_PATH=path/to/results_spikeglx \
    nextflow -C nextflow_local.config run main_local.nf --n_jobs 16 \
    --job_dispatch_args "--input spikeglx" --preprocessing_args "--debug --debug-duration 120"
```


## SLURM

To deploy on a SLURM cluster, you need to have access to a SLURM cluster and have the 
[Nextflow](https://www.nextflow.io/docs/latest/install.html) and Singularity/Apptainer installed. 
To use Figurl cloud visualizations, follow the same steps descrived in the 
[Local deployment - Requirements](#requirements) section and set the KACHERY environment variables.

Then, you can submit the pipeline to the cluster similarly to the Local deplyment, 
but wrapping the command into a script that can be launched with `sbatch`.

To avoid downloading the container images in the current location (usually the home folder),
you can set the `NXF_SINGULARITY_CACHEDIR` environment variable to a different location.

You can use the `slurm_submit.sh` script as a template to submit the pipeline to your cluster.
It is recommended to also make a copy of the `pipeline/nextflow_slurm.config` file and modify the `queue` parameter to match the partition you want to use on your cluster. In this example, we assume the copy is called 
`pipeline/nextflow_slurm_custom.config`.

```bash
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=4GB
#SBATCH --time=2:00:00
### change {your-partition} to the partition/queue on your cluster
#SBATCH --partition={your-partition}


# modify this section to make the nextflow command available to your environment
# e.g., using a conda environment with nextflow installed
conda activate env_nf

PIPELINE_PATH="path-to-your-cloned-repo"
DATA_PATH="path-to-data-folder"
RESULTS_PATH="path-to-results-folder"
WORKDIR="path-to-large-workdir"

NXF_VER=22.10.8 DATA_PATH=$DATA_PATH RESULTS_PATH=$RESULTS_PATH nextflow \
    -C $PIPELINE_PATH/pipeline/nextflow_slurm_custom.config \
    -log $RESULTS_PATH/nextflow/nextflow.log \
    run $PIPELINE_PATH/pipeline/main_slurm.nf \
    -work-dir $WORKDIR \
    --job_dispatch_args "--debug --debug-duration 120" \ # additional parameters
    -resume
```

> [!IMPORTANT]
> You should change the `--partition` parameter to match the partition you want to use on your cluster. 
> The same partition should be also indicated as the `queue` argument in the `pipeline/nextflow_slurm_custom.config` file!

Then, you can submit the script to the cluster with:

```bash
sbatch slurm_submit.sh
```

## Creating a custom layer for data ingestion

The default job-dispatch step only supports loading data 
from SpikeGLX, Open Ephys, NWB, and AIND formats.

To ingest other types of data, you can create a similar repo and modify the way that the job list is created 
(see the [job dispatch README](https://github.com/AllenNeuralDynamics/aind-ephys-job-dispatch/blob/main/README.md) for more details).

Then you can create a modified `main_local-slurm.nf`, where the `job_dispatch` process points to your custom job dispatch repo.

## Code Ocean (AIND)

At AIND, the pipeline is deployed on the Code Ocean platform. Since currently Code Ocean does not support conditional processes, pipelines running different sorters and AIND-specific options are implemented in separate branches.

This is a list of the available pipeline branches that are deployed in Code Ocean:

- `main`/`co_kilosort4`: pipeline with Kilosort4 sorter
- `co_kilosort25`: pipeline with Kilosort2.5 sorter
- `co_spykingcircus2`: pipeline with Spyking Circus 2 sorter
- `co_kilosort25_opto`: pipeline with Kilosort2.5 sorter and optogenetics artifact removal
- `co_kilosort4_opto`: pipeline with Kilosort4 sorter and optogenetics artifact removal
- `co_spykingcircus2_opto`: pipeline with Spyking Circus 2 sorter and optogenetics artifact removal